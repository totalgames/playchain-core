# Directory containing databases, logs, etc.
# data-dir = "playchain_node_data"

# Endpoint for P2P node to listen on
p2p-endpoint = 0.0.0.0:1777

# JSON array of P2P nodes to connect to on startup
# seed-nodes = 

# Pairs of [BLOCK_NUM,BLOCK_ID] that should be enforced as checkpoints.
# checkpoint = 

# Endpoint for websocket RPC to listen on
rpc-endpoint = 0.0.0.0:8090

# Endpoint for TLS websocket RPC to listen on
# rpc-tls-endpoint = 

# The TLS certificate file for this server
# server-pem = 

# Password for this certificate
# server-pem-password = 

# File to read Genesis State from
# genesis-json = 

# Block signing key to use for init witnesses, overrides genesis file
# dbg-init-key = 

# JSON file specifying API permissions
# api-access = 

# Space-separated list of plugins to activate
plugins = account_history

# Path to create a Genesis State at. If a well-formed JSON file exists at the path, it will be parsed and any missing fields in a Genesis State will be added, and any unknown fields will be removed. If no file or an invalid file is found, it will be replaced with an example Genesis State.
# create-genesis-json = 

# Rebuild object graph by replaying all blocks
# replay-blockchain = 

# Delete all blocks and re-sync with network from scratch
# resync-blockchain = 

# Force validation of all transactions
# force-validate = 

# Replace timestamp from genesis.json with current time plus this many seconds (experts only!)
# genesis-timestamp = 

# Enable block production, even if the chain is stale.
# enable-stale-production = false

# Percent of witnesses (0-99) that must be participating in order to produce blocks
required-participation = false

# ID of witness controlled by this node (e.g. "1.6.5", quotes are required, may specify multiple times)
# witness-id = 

# Tuple of [PublicKey, WIF private key] (may specify multiple times)
# private-key = 

# Tuple of [PublicKey, WIF private key] (may specify multiple times)
# debug-private-key =

# Account ID to track history for (may specify multiple times)
# track-account = 

# Keep only those operations in memory that are related to account history tracking
partial-operations = 1

# Maximum number of operations per account will be kept in memory
max-ops-per-account = 1000

# Track market history by grouping orders into buckets of equal size measured in seconds specified as a JSON array of numbers
bucket-size = [60,300,900,1800,3600,14400,86400]

# How far back in time to track history for each bucket size, measured in the number of buckets (default: 1000)
history-per-size = 1000

# Will only store this amount of matched orders for each market in order history for querying, or those meet the other option, which has more data (default: 1000)
max-order-his-records-per-market = 1000

# Will only store matched orders in last X seconds for each market in order history for querying, or those meet the other option, which has more data (default: 259200 (3 days))
max-order-his-seconds-per-market = 259200

# RPC endpoint of a trusted validating node (required)
# trusted-node = 

# Block number after which to do a snapshot
# snapshot-at-block = 

# Block time (ISO format) after which to do a snapshot
# snapshot-at-time = 

# Pathname of JSON file where to store the snapshot
# snapshot-to = 

# Group orders by percentage increase on price. Specify a JSON array of numbers here, each number is a group, number 1 means 0.01%. 
tracked-groups = [10,100]

# Block count perion, after which to do a snapshot
# backup-snapshot-block-period = 

# Pathname of JSON file where to store the snapshot
# backup-snapshot-dest = 

# >> elasticsearch PLUGIN
#
# Elastic Search database node url(http://localhost:9200/)
# elasticsearch-node-url =

# Number of bulk documents to index on replay(10000)
# elasticsearch-bulk-replay =

# Number of bulk documents to index on a syncronied chain(100)
# elasticsearch-bulk-sync = 

# Use visitor to index additional data(slows down the replay(false))
# elasticsearch-visitor =

# Pass basic auth to elasticsearch database('')
# elasticsearch-basic-auth = 

# Add a prefix to the index(playchain-)
# elasticsearch-index-prefix = 

# Save operation as object(false)
# elasticsearch-operation-object = 

# Start doing ES job after block(0)
# elasticsearch-start-es-after-block =

# >> es_objects PLUGIN
#
# Elasticsearch node url(http://localhost:9200/)
# es-objects-elasticsearch-url =

# Basic auth username:password('')
# es-objects-auth = 

# Number of bulk documents to index on replay(10000)
# es-objects-bulk-replay =

# Number of bulk documents to index on a synchronized chain(100)
# es-objects-bulk-sync = 

# Store proposal objects(true)
# es-objects-proposals =

# Store account objects(true)
# es-objects-accounts =

# Store asset objects(true)
# es-objects-assets =

# Store balances objects(true)
# es-objects-balances = 

# Store limit order objects(true)
# es-objects-limit-orders =

# Store feed data(true)
# es-objects-asset-bitasset =

# Add a prefix to the index(objects-)
# es-objects-index-prefix = 

# Keep only current state of the objects(true)
# es-objects-keep-only-current = 

# Start doing ES job after block(0)
# es-objects-start-es-after-block =

# <<

# declare an appender named "stderr" that writes messages to the console
[log.console_appender.stderr]
stream=std_error

[log.file_appender.p2p]
filename=logs/p2p/p2p.log

[log.file_appender.default]
filename=logs/all.log
# filename can be absolute or relative to this config file

# route any messages logged to the default logger to the "stderr" logger we
# declared above, if they are info level are higher
[logger.default]
level=debug
appenders=stderr default

# route messages sent to the "p2p" logger to the p2p appender declared above
[logger.p2p]
level=info
appenders=p2p


